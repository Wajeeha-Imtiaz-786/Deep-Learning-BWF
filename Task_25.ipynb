{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mbk62TkwGRzU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Define the number of classes\n",
        "num_classes = 10\n",
        "input_size=100\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(64, activation='relu', input_shape=(input_size,)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Example 1: Addressing underfitting\n",
        "# Increase the model complexity by adding more layers or neurons\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Example 2: Addressing overfitting\n",
        "# Add dropout layers to randomly deactivate some neurons during training\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Example 3: Regularization\n",
        "# Apply L1 or L2 regularization to the model weights\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.01)))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ]
    }
  ]
}